\section{Лекция 6 -- 2024-03-29 -- Основные характеристики случайного процесса}
\subsection{Конечномерные распределения}
\begin{definition}
  \emph{Конечномерным распределением} случайного процесса $\xi_t$, $t\in T$ 
  называется совокупность законов распределения
  (с любым $ N $) $t_1 < t_2 < \dots < t_N$, $t_i \in T$
  случайных величин $\bm{\xi} = (\xi_{t_1}, \xi_{t_2}, \dots, \xi_{t_N})$.

  Функция распределения
  $F_{\bm{\xi}} (\mathbf{x}) = P(\xi_{t_1} < x_1, \xi_{t_2} < x_2, \dots, \xi_{t_N} < x_N)$
  называется \emph{конечномерной функцией распределения}.
\end{definition}


\begin{theorem}
  Если $\xi_t$ --- марковская цепь, то его конечномерное распределение однозначно 
  определено одно- или двумерным распределениями.
\end{theorem}
\begin{proof}
  Пусть $t_1 < t_2 < \dots < t_N$. Тогда
  \begin{multline*}
    F_{\bm{\xi}} (\mathbf{x}) = P(\xi_{t_1} < x_1, \dots, \xi_{t_N} < x_N) = \\
    = P(\xi_{t_N} < x_N \mid \xi_{t_{N-1}} < x_{N-1}, \dots, \xi_{t_1} < x_1)
      P(\xi_{t_{N-1}} < x_{N}, \dots, \xi_{t_1} < x_1) = \\
    = P(\xi_{t_N} < x_N \mid \xi_{t_{N-1}} < x_{N-1}) P(\xi_{t_{N-1}} < x_{N-1}).
  \end{multline*}
  Таким образом, любое конечномерное распределение определяется такими вероятностями.
\end{proof}

\begin{definition}
  Если
  \[
    F_{\bm{\xi}}(\mathbf{x}) = \int\limits_{-\infty}^{\bar{x}} p_{\bar{\xi}}
    (\bar{y}) \, dy,\quad \text{то }     p_{\bm{\xi}} (\mathbf{x}) = \dfrac{\partial^N F_{\bar{\xi}}
    (\bar{x})}{\partial x_1 \partial x_2 \dots \partial x_N},
  \]
  и $\bar{p}_\bar{\xi} (\bar{x})$ --- \emph{конечномерная плотность}.
\end{definition}

\begin{ex}
  Пусть случаный процесс $\xi_t (\omega) = U(\omega)\varphi(t)$, где $ U(\omega)
  $ --- некоторая случайная величина, а $ \varphi(t) > 0 $ --- неслучайная
  положительная функция. Тогда если $p_U (x)$ --- плотность случайной величины $U$,
  то
  \[
    p_{\xi_t} (x) = p_U \left(\dfrac{x}{\varphi(t)}\right) 
    \dfrac{1}{\varphi(t)}
  \]
  ---
  одномерная плотность.

  Рассмотрим двумерные функции распределения $t_1 < t_2$
  (обозначим $(\xi_1, \xi_2) = (\xi_{t_1}, \xi_{t_2})$). Тогда
  \begin{multline*}
    F_{\xi_1, \xi_2} (x_1, x_2) = P(\xi_1 < x_1, \xi_2 < x_2)
    = P\left(U\varphi(t_1) < x_1, U\cdot\varphi(t_2) < x_2\right) = \\
    = P\left(U < \min \left(\dfrac{x_1}{\varphi(t_1)},
      \dfrac{x_2}{\varphi(t_2)}\right)\right)
    = F_U \left( \dfrac{x_1}{\varphi(t_1)}, \dfrac{x_2}{\varphi(t_2)} \right)
  \end{multline*}
  .

  Вне прямой $x_1/\varphi(t_1) = x_2/\varphi(t_2)$ выполняется
  \[
    \dfrac{\partial^2 F}{\partial x_1 \partial x_2} = 0,
  \]
  то есть вся вероятностная
  мера сосредоточена на этой прямой, и не существует двумерной плотности.
\end{ex}

\begin{definition}
  Случайный процесс называется \emph{гауссовским}, если все его конечномерные распределения
  --- гауссовские, то есть $\bm\xi = (\xi_{t_1}, \xi_{t_2}, \dots, \xi_{t_N})$ --
  гауссовски распределенный случайный вектор.
\end{definition}

Если известно $M\xi_t = m_\xi (t)$ и ковариационная матрица
$\Sigma_\xi = \left(\cov(\xi_{t_i}, \xi_{t_j})\right)_{ij}$ не вырождена,
то можно получить и плотность:
\[
  p_{\bm{\xi}} (\mathbf{x}) = \dfrac{1}{(2\pi)^{N/2} \sqrt{|\Sigma_\xi|}}
  \exp\left\{ -\dfrac{1}{2} (\mathbf{x} - M\bm{\xi})^{\mathsf T} \Sigma^{-1}_\xi
    (\mathbf{x} -
  M\bm{\xi}) \right\}.
\]
Такой случайный процесс практически единственный, который можно потрогать.

\paragraph{Характеристики случайного процесса.}
\begin{enumerate}
  \item $M\xi_t(\omega) = m_\xi(t)$;
  \item $k_\xi (t, s) = \cov(\xi_t, \xi_s)$ -- ковариационная функция.
    Её свойства:
    \begin{enumerate}
      \item $k_\xi (t, s) = k_\xi (s, t)$,
      \item $k_\xi (t, s) \leqslant \sqrt{D\xi_t D\xi_s}$,
      \item $k_\xi(t, t) = D\xi_t$,
      \item \textsc{Неотрицательная определенность}:
        \[
          \forall t_1, t_2, \dots, t_N, z_1, \dots, z_N \in \mathbb{C} : \sum_{i, j = 1}^N z_i k_\xi(t_i, t_j) \bar{z_j} \geqslant 0
        \]
        \begin{proof}
          $\zeta = \sum\limtis_{j=1}^N z_j \xi_{t_j}$, и
          \[
            0 \leqslant \cov(\zeta, \bar{\zeta}) = \cov\biggl(\sum_{j=1}^N z_j
            \xi_{t_j}, \sum \dots\biggr) = \sum_{i, j} z_i \bar{z_j} k_\xi(t_i, t_j).
          \]
        \end{proof}
    \end{enumerate}
  \item Нормированная (авто)ковариационная функция.
\end{enumerate}

\begin{ex}[пример ковариационной функции]
  $k_\xi(t, s) = \cos(t-s)$ --- коварицанионная функция.
\end{ex}

\begin{ex}
  Рассмотрим случайный процесс $\xi_t = U \cdot \cos(\omega t) + V \sin(\omega t)$,
  где $U, V$ -- независимые случайные величины:
  \begin{align*}
    M\xi_t &= MU \cdot \cos(\omega t) + MV \cdot \sin(\omega t), \\
    k_{\xi}(t, s)
           &= \cov \left( U \cdot \cos(\omega t) + V \sin(\omega t), U \cdot \cos(\omega s) + V \sin(\omega s) \right) 
    = DU \cdot \cos(\omega t) \cos(\omega s) + DV \sin(\omega t) \sin(\omega s).
  \end{align*}
\end{ex}


\begin{definition}
  \emph{Разностным оператором} называется функция $ \Delta_{a_ib_i}\colon
  \mathbb{R}^n \to \mathbb{R} $, действующая по формуле ($ a_i \leqslant b_i
  $) 
  \[
    \Delta_{a_ib_i} F_n(x_1, \ldots, x_n) = F_n(x_1, \ldots, x_{i-1}, b_i,
    x_{i+1}, \ldots, x_n) - F_n(x_1, \ldots, x_{i-1}, a_i, x_{i+1}, x_n).
  \]
  Простой подсчёт показывает, что если  
  \[
    F_n(\mathbf{x}) = \mathsf P\bigg(\bigtimes_{i=1}^n (-\infty, x_i]\bigg),
  \]
  то и  
  \[
    \Delta_{a_1b_1}\dots\Delta_{a_nb_n} F_n(x_1, x_2, \dots, x_n) = \mathsf P(a,
    b] \neq F_n(b) - F_n(a).
  \] 
  
\end{definition}

Перечислим свойства многомерной функции распределения.
  \begin{enumerate}
    \item $0 \leqslant F_{\mathbf{t}} (\mathbf{x}) \leqslant 1$.
    \item $\lim\limits_{|x| \to -\infty} F_{\mathbf{t}} (\mathbf{x}) = 0$,
      $\lim\limits_{|x|\to +\infty} F_{\mathbf{t}}(\mathbf{x}) = 1$.
    \item $F_{\mathbf{t}} (\mathbf{x})$ непрерывна слева по каждому аргументу.
    \item Пусть $(i_1, \dots, i_N)$ есть перестановка $(1, 2, \dots, N)$.
      $F_{t_{i_1}, \dots, t_{i_N}} (x_{i_1}, \dots x_{i_N})
      = F_{t_1, \dots, t_N} (x_1, \dots, x_N).$
    \item $k < N \colon F_{t_1, \dots, t_k} (x_1, \dots, x_k) = F_{\mathbf{t}} (x_1,
      \dots, x_k, +\infty, \dots, +\infty)$.
    \item $\Delta_{h_1, \dots, h_N} F_{\mathbf{t}} (x_1, \dots, x_N) \geqslant
      0, h_i > 0$,
      $\Delta_h F = F_{\mathbf{t}} (x_1+h, x_2, \dots, x_N) - F_{\mathbf{t}}
      (x_1, \dots, x_N)$.
      $\Delta_{h_1, h_2} F = F_\mathbf{t} $.
  \end{enumerate}

\begin{theorem}[Колмогоров]
  Если семейство функций $F_{\bar{t}} (\bar{x})$ удовлетворяет шести свойствам
  выше,
  то существуют $ (\Omega, \mathcal{F}, P)$ и $\xi_t$ такие, что $F_{\bar{t}} (\bar{x})$ --
  конечномерные распределения случайного процесса $\xi_t$.
\end{theorem}

\subsection{Стационарные процессы}

\paragraph{Определения стационарного процесса}

\begin{definition}
  Случайный процесс называется \emph{стационарным в узком смысле}, если его конечномерные
  распределения не меняюются при сдвиге:
  \[
    F_{t_1, \dots, t_N}(x_1, \dots, x_N) = F_{t_1+h, \dots, t_N+h} (x_1, \dots, x_N).
  \]
\end{definition}

\begin{ex}
  $(\alpha, \beta)$ -- неотрицательные случайные величины с $p_{\alpha\beta} (x, y)$,
  $\gamma\sim R[0, 2\pi]$, $\gamma$ и $\alpha,\beta$ -- независимы.
  Тогда $\xi_t = \alpha \cos(\beta t + \gamma)$ -- стационарный в узком смысле.

  \begin{multline*}
    F_{\xi_t}(x)
    = P(\alpha \cos(\beta t + \gamma) < x)
    = P(\cos(\beta t + \gamma) < \dfrac{x}{\alpha}) = \\
    = P\left( \bigcup_k \left( \arccos\dfrac{x}{\alpha} - \beta t + 2\pi k < \gamma < 2\pi +2\pi k - \arccos\dfrac{x}{\alpha} - \beta t \right) \right)
  \end{multline*}
  -- не зависит от $t$ (потому что геометрически, мы просто смещаем эти интервалы на $-\beta t$,
  но так каждое такое событие периодическое, сумма остаётся одинаковой).
  
\end{ex}

\begin{definition}
  Случайный процесс называется \emph{стационарным в широком смысле}, если 
  $M\xi_t = m_\xi(t) \equiv C$, $k_\xi(t, s) = k_\xi(t-s)$
\end{definition}

\begin{definition}
  Нормированная (авто) ковариационная (также называется корреляционной) функция:
  $r_{\xi_t, \xi_s} = \dfrac{cov(\xi_t, \xi_s)}{\sqrt{D\xi_t \cdot D\xi_s}}$
\end{definition}

\begin{theorem}[о связи стационарного процесса в широком и узком смыслах]
  Если случайный процесс $\xi_t$ стационарен в узком смысле и $\exists M\xi_t, M\xi^2_t$, 
  то $\xi_t$ стационарем в широком смысле.
\end{theorem}
\begin{proof}
  $M\xi_t = \int\limits_{-\infty}^{+\infty} x dF_{\xi_t} (x) = M\xi_{t+h}$

  $M\xi_t \xi_s = \int\int xy dF_{\xi_t \xi_s} (x, y) = \int\int xy dF_{\xi_{t+h}, \xi_{s+h}} (x, y)$

  $cov(\xi_{t+h}, \xi_{s+h}) = M\xi_{t+h} \xi_{s+h} - M\xi_{t+h} \cdot M\xi_{s+h} = cov(\xi_t, \xi_s)$
\end{proof}



\paragraph{Свойства ковариационной функции стационарного процесса (в широком смысле)}

\begin{enumerate}
  \item $k_\xi(-\tau) = k_\xi (\tau)$
  \item $|k_\xi(\tau)| \leqslant k_\xi(0) = D_{\xi_t}$
  \item неотрицательная определенность
\end{enumerate}

\begin{ex}[Пуассоновский процесс]
  Пусть $\xi_t$ -- пуассоновский процесс интенсивности $\lambda$, тогда
  \[
    M\xi_t = \sum_{k=0}^\infty k P(\xi_t = k) = \sum_{k=0}^\infty k \dfrac{(\lambda t)^k}{k!} e^{-\lambda t} = \lambda t.
  \]

  $t<s$:
  \begin{multline*}
    M\xi_t \xi_s = \sum_{k=0}^\infty \sum_{j=k}^\infty k\cdot j \cdot P(\xi_t = k, \xi_s = j)
    = \sum_{k=0}^\infty \sum_{j=k}^\infty k \cdot j \cdot P(\xi_s=j | \xi_t=k) \cdot P(\xi_t = k) = \\
    = \sum_{k=0}^\infty \sum_{j=k}^\infty kj P_{kj}(s-t) \cdot \dfrac{(\lambda t)^k}{k!} e^{-\lambda t}
    = \sum_{k=0}^\infty \sum_{j=k}^\infty kj P_{kj}(s-t) \cdot \dfrac{(\lambda t)^k}{k!} e^{-\lambda t} = \\
    = \sum_{k=0}^\infty k \dfrac{(\lambda t)^k}{k!} e^{-\lambda t}
    \sum_{j=k}^\infty (j-k+k) \dfrac{(\lambda(s-t))^{j-k}}{(j-k)!} e^{- \lambda (s-t)}
    = \sum_{k=0}^\infty k \dfrac{(\lambda t)^k}{k!} e^{-\lambda t} [\lambda(s-t) + k] = \\
    = \lambda t \lambda (s-t) + (\lambda t)^2 + \lambda t = \lambda^2 t s + \lambda t
  \end{multline*}

  $cov(\xi_t, \xi_s) = \lambda t$ $\Rightarrow$ $t < s : k_\xi (t, s) = \lambda \min(t, s)$
\end{ex}
